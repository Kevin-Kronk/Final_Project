[
  {
    "objectID": "EDA.html",
    "href": "EDA.html",
    "title": "ST 558 Final Project EDA",
    "section": "",
    "text": "The Diabetes Health Indicators Dataset is from the BRFSS 2015 dataset on Kaggle. It is the Behavioral Risk Factor Surveillance System, which is a telephone survey collected annually by the CDC. Diabetes is a chronic disease that is prevalent in the United States, has a strongly negative impact on those with it, and has a major negative impact on the economy. Early diagnosis of the disease can allow individuals to make lifestyle changes and seek treatment, therefore it is important to build a model that can accurately predict it.\nThe focus of my project will be on how lifestyle and basic health variables impact whether or not someone has diabetes and if we are able to predict having diabetes based on these variables. The EDA will be used to explore these variables to look at their frequencies and distributions, as well as see how they relate to the response variable Diabetes_binary which expresses whether the person has diabetes or not. Modeling will be done to determine which model is best able to predict someone having diabetes based on which has the lowest log-loss.\n\n\nDiabetes_binary - binary variable where 0 is no diabetes and 1 is for prediabetes or diabetes. This is the response variable.\nHighBP - binary variable where 0 is not high and 1 is high blood pressure.\nSmoker - binary variable asking if the person has smoked at least 100 cigarettes (5 packs) in their entire life, where 0 is for no and 1 for yes.\nPhysActivity - binary variable for if they’ve done physical activity in the past 30 days, not including their job, where 0 is no and 1 is yes.\nFruits - binary variable for if they consume 1 or more fruit per day, where 0 is no and 1 is yes.\nVeggies - binary variable for if they consume 1 or more vegetable per day, where 0 is no and 1 is yes.\nHvyAlcoholConsump - binary variable for if men have &gt;= 14 drinks or women &gt;= 7 drinks per week, where 0 is no and 1 is yes.\nGenHlth - ordinal categorical variable describing their general health, where 1 is excellent, 2 is very good, 3 is good, 4 is fair, and 5 is poor.\nDiffWalk - binary variable for if they have serious difficulty walking or climbing stairs, where 0 is no and 1 is yes.\nSex - binary variable where 0 is female and 1 is male.\nAge - numerical variable from 1 to 13 representing different age categories where 1 is 18-24, 9 is 60-64, and 13 is 80 or older.\nIncome - numerical variable representing different income categories where 1 is less than $10,000, 5 is less than $35,000, and 8 is $75,000 or more.\n\n\n\n\n# Load in libraries\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.2.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(DescTools)\n\n\n# Load in Diabetes Health Indicators Dataset\ndiabetes_data &lt;- read_csv(\"API_Docker/diabetes_binary_health_indicators_BRFSS2015.csv\",\n                          show_col_types = FALSE)\n\n# Select only the variables used for my analysis and convert binary to factor\nanalysis_variables &lt;- c(\"Diabetes_binary\", \"HighBP\", \"Smoker\", \"PhysActivity\",\n                        \"Fruits\", \"Veggies\", \"HvyAlcoholConsump\", \"GenHlth\",\n                        \"DiffWalk\", \"Sex\", \"Age\", \"Income\")\n\n# When converting variables to factor, set levels and labels for those levels\ndiabetes_data &lt;- diabetes_data |&gt;\n  select(all_of(analysis_variables)) |&gt;\n  mutate(Diabetes_binary = factor(Diabetes_binary, \n                                  levels = c(\"0\", \"1\"), labels = c(\"No\", \"Yes\")),\n         HighBP = factor(HighBP,\n                         levels = c(\"0\", \"1\"), labels = c(\"Low\", \"High\")),\n         Smoker = factor(Smoker,\n                         levels = c(\"0\", \"1\"), labels = c(\"No\", \"Yes\")),\n         PhysActivity = factor(PhysActivity,\n                               levels = c(\"0\", \"1\"), labels = c(\"No\", \"Yes\")),\n         Fruits = factor(Fruits,\n                         levels = c(\"0\", \"1\"), labels = c(\"No\", \"Yes\")), \n         Veggies = factor(Veggies,\n                          levels = c(\"0\", \"1\"), labels = c(\"No\", \"Yes\")),\n         HvyAlcoholConsump = factor(HvyAlcoholConsump,\n                                    levels = c(\"0\", \"1\"), labels = c(\"No\", \"Yes\")),\n         DiffWalk = factor(DiffWalk,\n                           levels = c(\"0\", \"1\"), labels = c(\"No\", \"Yes\")),\n         Sex = factor(Sex,\n                      levels = c(\"0\", \"1\"), labels = c(\"Female\", \"Male\")))\n\ndiabetes_data\n\n# A tibble: 253,680 × 12\n   Diabetes_binary HighBP Smoker PhysActivity Fruits Veggies HvyAlcoholConsump\n   &lt;fct&gt;           &lt;fct&gt;  &lt;fct&gt;  &lt;fct&gt;        &lt;fct&gt;  &lt;fct&gt;   &lt;fct&gt;            \n 1 No              High   Yes    No           No     Yes     No               \n 2 No              Low    Yes    Yes          No     No      No               \n 3 No              High   No     No           Yes    No      No               \n 4 No              High   No     Yes          Yes    Yes     No               \n 5 No              High   No     Yes          Yes    Yes     No               \n 6 No              High   Yes    Yes          Yes    Yes     No               \n 7 No              High   Yes    No           No     No      No               \n 8 No              High   Yes    Yes          No     Yes     No               \n 9 Yes             High   Yes    No           Yes    Yes     No               \n10 No              Low    No     No           No     Yes     No               \n# ℹ 253,670 more rows\n# ℹ 5 more variables: GenHlth &lt;dbl&gt;, DiffWalk &lt;fct&gt;, Sex &lt;fct&gt;, Age &lt;dbl&gt;,\n#   Income &lt;dbl&gt;\n\n\n\n\n\n\nstr(diabetes_data)\n\ntibble [253,680 × 12] (S3: tbl_df/tbl/data.frame)\n $ Diabetes_binary  : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 2 1 ...\n $ HighBP           : Factor w/ 2 levels \"Low\",\"High\": 2 1 2 2 2 2 2 2 2 1 ...\n $ Smoker           : Factor w/ 2 levels \"No\",\"Yes\": 2 2 1 1 1 2 2 2 2 1 ...\n $ PhysActivity     : Factor w/ 2 levels \"No\",\"Yes\": 1 2 1 2 2 2 1 2 1 1 ...\n $ Fruits           : Factor w/ 2 levels \"No\",\"Yes\": 1 1 2 2 2 2 1 1 2 1 ...\n $ Veggies          : Factor w/ 2 levels \"No\",\"Yes\": 2 1 1 2 2 2 1 2 2 2 ...\n $ HvyAlcoholConsump: Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n $ GenHlth          : num [1:253680] 5 3 5 2 2 2 3 3 5 2 ...\n $ DiffWalk         : Factor w/ 2 levels \"No\",\"Yes\": 2 1 2 1 1 1 1 2 2 1 ...\n $ Sex              : Factor w/ 2 levels \"Female\",\"Male\": 1 1 1 1 1 2 1 1 1 2 ...\n $ Age              : num [1:253680] 9 7 9 11 11 10 9 11 9 8 ...\n $ Income           : num [1:253680] 3 1 8 6 4 8 7 4 1 3 ...\n\n\nThere are 253,680 observations with 1 response variable and 11 predictor variables.\n\n\n\n\n# Check for missing values\ncolSums(is.na(diabetes_data))\n\n  Diabetes_binary            HighBP            Smoker      PhysActivity \n                0                 0                 0                 0 \n           Fruits           Veggies HvyAlcoholConsump           GenHlth \n                0                 0                 0                 0 \n         DiffWalk               Sex               Age            Income \n                0                 0                 0                 0 \n\n\nThere are no missing values for these variables.\n\n\n\n\n# Get the counts of the Diabetes_binary values\ndiabetes_data |&gt;\n  group_by(Diabetes_binary) |&gt;\n  summarize(count = n())\n\n# A tibble: 2 × 2\n  Diabetes_binary  count\n  &lt;fct&gt;            &lt;int&gt;\n1 No              218334\n2 Yes              35346\n\n\n\n# Bar plot of Diabetes_binary\nggplot(diabetes_data, aes(Diabetes_binary, fill = Diabetes_binary)) +\n  geom_bar() + \n  labs(title = \"Bar Plot of Diabetes Binary\", \n       x = \"Diabetes Binary\", fill = \"Diabetes Binary\")\n\n\n\n\n\n\n\n\nAs you can see there are far more people who do not have diabetes than those who either have prediabetes or diabetes. Therefore when splitting the data into a training and test set, this response variable will be used as a strata to make sure that both sets have a roughly equal proportion of “No” and “Yes”.\n\n\n\n\n# Get the counts of the HighBP values\ndiabetes_data |&gt;\n  group_by(HighBP) |&gt;\n  summarize(count = n())\n\n# A tibble: 2 × 2\n  HighBP  count\n  &lt;fct&gt;   &lt;int&gt;\n1 Low    144851\n2 High   108829\n\n\n\n# Bar plot of HighBP\nggplot(diabetes_data, aes(HighBP, fill = HighBP)) +\n  geom_bar() +\n  labs(title = \"Bar Plot of High Blood Pressure\", \n       x = \"High Blood Pressure\", fill = \"High BP\")\n\n\n\n\n\n\n\n\n\n# Get two way contingency table of HighBP and Diabetes_binary\ndiabetes_data |&gt;\n  group_by(HighBP, Diabetes_binary) |&gt;\n  summarize(count = n(), .groups = \"drop\")\n\n# A tibble: 4 × 3\n  HighBP Diabetes_binary  count\n  &lt;fct&gt;  &lt;fct&gt;            &lt;int&gt;\n1 Low    No              136109\n2 Low    Yes               8742\n3 High   No               82225\n4 High   Yes              26604\n\n\n\n# Plot of HighBP by Diabetes_binary\nggplot(diabetes_data, aes(HighBP, fill = HighBP)) +\n  geom_bar() +\n  facet_wrap(~ Diabetes_binary) + \n  labs(title = \"Bar Plots of High Blood Pressure by Diabetes Diagnosis\",\n       x = \"High Blood Pressure\", fill = \"High BP\")\n\n\n\n\n\n\n\n\nOverall there are more people with low blood pressure, than with high blood pressure. However, when looking across the diabetes diagnosis, it appears that there are more people who have high blood pressure than low in the group that also has diabetes.\n\n\n\n\n# Get the counts of the Smoker values\ndiabetes_data |&gt;\n  group_by(Smoker) |&gt;\n  summarize(count = n())\n\n# A tibble: 2 × 2\n  Smoker  count\n  &lt;fct&gt;   &lt;int&gt;\n1 No     141257\n2 Yes    112423\n\n\n\n# Bar plot of Smoker\nggplot(diabetes_data, aes(Smoker, fill = Smoker)) +\n  geom_bar() + \n  labs(title = \"Bar Plot of Smoking Status\")\n\n\n\n\n\n\n\n\n\n# Get two way contingency table of Smoker and Diabetes_binary\ndiabetes_data |&gt;\n  group_by(Smoker, Diabetes_binary) |&gt;\n  summarize(count = n(), .groups = \"drop\")\n\n# A tibble: 4 × 3\n  Smoker Diabetes_binary  count\n  &lt;fct&gt;  &lt;fct&gt;            &lt;int&gt;\n1 No     No              124228\n2 No     Yes              17029\n3 Yes    No               94106\n4 Yes    Yes              18317\n\n\n\n# Plot of Smoker by Diabetes_binary\nggplot(diabetes_data, aes(Smoker, fill = Smoker)) +\n  geom_bar() +\n  facet_wrap(~ Diabetes_binary) + \n  labs(title = \"Bar Plots of Smoking Status by Diabetes Diagnosis\")\n\n\n\n\n\n\n\n\nOverall, there appears to be more people who are not smokers than those who are. However, when looking by diabetes diagnosis, the group that has diabetes have slightly more smokers than non-smokers.\n\n\n\n\n# Get the counts of the PhysActivity values\ndiabetes_data |&gt;\n  group_by(PhysActivity) |&gt;\n  summarize(count = n())\n\n# A tibble: 2 × 2\n  PhysActivity  count\n  &lt;fct&gt;         &lt;int&gt;\n1 No            61760\n2 Yes          191920\n\n\n\n# B plot of PhysActivity\nggplot(diabetes_data, aes(PhysActivity, fill = PhysActivity)) +\n  geom_bar() + \n  labs(title = \"Bar Plot of Physical Activity\", \n       x = \"Physical Activity\", fill = \"Physical Activity\")\n\n\n\n\n\n\n\n\n\n# Get two way contingency table of PhysActivity and Diabetes_binary\ndiabetes_data |&gt;\n  group_by(PhysActivity, Diabetes_binary) |&gt;\n  summarize(count = n(), .groups = \"drop\")\n\n# A tibble: 4 × 3\n  PhysActivity Diabetes_binary  count\n  &lt;fct&gt;        &lt;fct&gt;            &lt;int&gt;\n1 No           No               48701\n2 No           Yes              13059\n3 Yes          No              169633\n4 Yes          Yes              22287\n\n\n\n# Plot of PhysActivity by Diabetes_binary\nggplot(diabetes_data, aes(PhysActivity, fill = PhysActivity)) +\n  geom_bar() +\n  facet_wrap(~ Diabetes_binary) + \n  labs(title = \"Bar Plots of Physical Activity by Diabetes Diagnosis\",\n       x = \"Physical Activity\", fill = \"Physical Activity\")\n\n\n\n\n\n\n\n\nOverall there are far more people who get physical activity outside of work than those who don’t. Breaking it down by diabetes diagnosis, the same trend can be seen in both groups, however the proportion of those who exercise compared to those who don’t appears to be lower in the diabetes group.\n\n\n\n\n# Get the counts of the Fruits values\ndiabetes_data |&gt;\n  group_by(Fruits) |&gt;\n  summarize(count = n())\n\n# A tibble: 2 × 2\n  Fruits  count\n  &lt;fct&gt;   &lt;int&gt;\n1 No      92782\n2 Yes    160898\n\n\n\n# Bar plot of Fruits\nggplot(diabetes_data, aes(Fruits, fill = Fruits)) +\n  geom_bar() + \n  labs(title = \"Bar Plot of Fruit Consumption\")\n\n\n\n\n\n\n\n\n\n# Get two way contingency table of Fruits and Diabetes_binary\ndiabetes_data |&gt;\n  group_by(Fruits, Diabetes_binary) |&gt;\n  summarize(count = n(), .groups = \"drop\")\n\n# A tibble: 4 × 3\n  Fruits Diabetes_binary  count\n  &lt;fct&gt;  &lt;fct&gt;            &lt;int&gt;\n1 No     No               78129\n2 No     Yes              14653\n3 Yes    No              140205\n4 Yes    Yes              20693\n\n\n\n# Plot of Fruits by Diabetes_binary\nggplot(diabetes_data, aes(Fruits, fill = Fruits)) +\n  geom_bar() +\n  facet_wrap(~ Diabetes_binary) +\n  labs(title = \"Bar Plots of Fruit Consumption by Diabetes Diagnosis\")\n\n\n\n\n\n\n\n\nOverall there are more people who consume fruits each day than those that don’t. Breaking it down by diabetes diagnosis and the two groups look roughly similar. Fruit consumption likely doesn’t relate much to diabetes diagnosis.\n\n\n\n\n# Get the counts of the Veggies values\ndiabetes_data |&gt;\n  group_by(Veggies) |&gt;\n  summarize(count = n())\n\n# A tibble: 2 × 2\n  Veggies  count\n  &lt;fct&gt;    &lt;int&gt;\n1 No       47839\n2 Yes     205841\n\n\n\n# Bar plot of Veggies\nggplot(diabetes_data, aes(Veggies, fill = Veggies)) +\n  geom_bar() +\n  labs(title = \"Bar Plot of Vegetable Consumption\")\n\n\n\n\n\n\n\n\n\n# Get two way contingency table of Veggies and Diabetes_binary\ndiabetes_data |&gt;\n  group_by(Veggies, Diabetes_binary) |&gt;\n  summarize(count = n(), .groups = \"drop\")\n\n# A tibble: 4 × 3\n  Veggies Diabetes_binary  count\n  &lt;fct&gt;   &lt;fct&gt;            &lt;int&gt;\n1 No      No               39229\n2 No      Yes               8610\n3 Yes     No              179105\n4 Yes     Yes              26736\n\n\n\n# Plot of Veggies by Diabetes_binary\nggplot(diabetes_data, aes(Veggies, fill = Veggies)) +\n  geom_bar() +\n  facet_wrap(~ Diabetes_binary) + \n  labs(title = \"Bar Plots of Vegetable Consumption by Diabetes Diagnosis\")\n\n\n\n\n\n\n\n\nThere are more people who consume veggies each day than those that don’t. Breaking it down by diabetes diagnosis and the two groups look roughly similar. Veggie consumption likely doesn’t relate much to diabetes diagnosis.\n\n\n\n\n# Get the counts of the HvyAlcoholConsump values\ndiabetes_data |&gt;\n  group_by(HvyAlcoholConsump) |&gt;\n  summarize(count = n())\n\n# A tibble: 2 × 2\n  HvyAlcoholConsump  count\n  &lt;fct&gt;              &lt;int&gt;\n1 No                239424\n2 Yes                14256\n\n\n\n# Bar plot of HvyAlcoholConsump\nggplot(diabetes_data, aes(HvyAlcoholConsump, fill = HvyAlcoholConsump)) +\n  geom_bar() +\n  labs(title = \"Bar Plot of Heavy Alcohol Consumption\",\n       x = \"Heavy Alcohol Consumption\", fill = \"Hvy Alcohol Consump\")\n\n\n\n\n\n\n\n\n\n# Get two way contingency table of HvyAlcoholConsump and Diabetes_binary\ndiabetes_data |&gt;\n  group_by(HvyAlcoholConsump, Diabetes_binary) |&gt;\n  summarize(count = n(), .groups = \"drop\")\n\n# A tibble: 4 × 3\n  HvyAlcoholConsump Diabetes_binary  count\n  &lt;fct&gt;             &lt;fct&gt;            &lt;int&gt;\n1 No                No              204910\n2 No                Yes              34514\n3 Yes               No               13424\n4 Yes               Yes                832\n\n\n\n# Plot of HvyAlcoholConsump by Diabetes_binary\nggplot(diabetes_data, aes(HvyAlcoholConsump, fill = HvyAlcoholConsump)) +\n  geom_bar() +\n  facet_wrap(~ Diabetes_binary) + \n  labs(title = \"Bar Plots of Heavy Alcohol Consumption by Diabetes Diagnosis\",\n       x = \"Heavy Alcohol Consumption\", fill = \"Hvy Alcohol Consump\")\n\n\n\n\n\n\n\n\nThere are more people who don’t consume a heavy amount of alcohol than those who do. Breaking it down by diabetes diagnosis and the two groups look roughly similar. Heavy alcohol consumption likely doesn’t relate much to diabetes diagnosis.\n\n\n\n\n# Get the counts of the GenHlth values\ndiabetes_data |&gt;\n  group_by(GenHlth) |&gt;\n  summarize(count = n())\n\n# A tibble: 5 × 2\n  GenHlth count\n    &lt;dbl&gt; &lt;int&gt;\n1       1 45299\n2       2 89084\n3       3 75646\n4       4 31570\n5       5 12081\n\n\n\n# Get the summary statistics of GenHlth\nsummary(diabetes_data$GenHlth)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   2.000   2.000   2.511   3.000   5.000 \n\n\n\n# Bar plot of GenHlth\nggplot(diabetes_data, aes(GenHlth, fill = factor(GenHlth))) +\n  geom_bar() +\n  labs(title = \"Bar Plot of General Health\",\n       x = \"General Health (Excellent to Poor)\", fill = \"General Health\")\n\n\n\n\n\n\n\n\n\n# Get mean of GenHlth for those with and without diabetes\ndiabetes_data |&gt;\n  group_by(Diabetes_binary) |&gt;\n  summarize(Mean = mean(GenHlth), .groups = \"drop\")\n\n# A tibble: 2 × 2\n  Diabetes_binary  Mean\n  &lt;fct&gt;           &lt;dbl&gt;\n1 No               2.39\n2 Yes              3.29\n\n\n\n# Boxplots of GenHlth by Diabetes_binary\nggplot(diabetes_data, aes(Diabetes_binary, GenHlth, color = Diabetes_binary)) +\n  geom_boxplot() + \n  labs(title = \"Boxplots of General Health by Diabetes Diagnosis\",\n       x = \"Diabetes Binary\", y = \"General Health\", color = \"Diabetes Binary\")\n\n\n\n\n\n\n\n\n\n# Plot of GenHlth by Diabetes_binary\nggplot(diabetes_data, aes(GenHlth, fill = factor(GenHlth))) +\n  geom_bar() +\n  facet_wrap(~ Diabetes_binary) +\n  labs(title = \"Bar Plots of General Health by Diabetes Diagnosis\",\n       x = \"General Health\", fill = \"General Health\")\n\n\n\n\n\n\n\n\nOn average the general health of those with diabetes is worse than those without diabetes. As we can see in the distribution, there are proportionally far fewer people in excellent (1), very good (2), and good health (3), in the diabetes group.\n\n\n\n\n# Get the counts of the DiffWalk values\ndiabetes_data |&gt;\n  group_by(DiffWalk) |&gt;\n  summarize(count = n())\n\n# A tibble: 2 × 2\n  DiffWalk  count\n  &lt;fct&gt;     &lt;int&gt;\n1 No       211005\n2 Yes       42675\n\n\n\n# Plot of DiffWalk\nggplot(diabetes_data, aes(DiffWalk, fill = DiffWalk)) +\n  geom_bar() +\n  labs(title = \"Bar Plot of Difficulty Walking\", \n       x = \"Difficulty Walking\", fill = \"Diff Walking\")\n\n\n\n\n\n\n\n\n\n# Get two way contingency table of DiffWalk and Diabetes_binary\ndiabetes_data |&gt;\n  group_by(DiffWalk, Diabetes_binary) |&gt;\n  summarize(count = n(), .groups = \"drop\")\n\n# A tibble: 4 × 3\n  DiffWalk Diabetes_binary  count\n  &lt;fct&gt;    &lt;fct&gt;            &lt;int&gt;\n1 No       No              188780\n2 No       Yes              22225\n3 Yes      No               29554\n4 Yes      Yes              13121\n\n\n\n# Plot of DiffWalk by Diabetes_binary\nggplot(diabetes_data, aes(DiffWalk, fill = factor(DiffWalk))) +\n  geom_bar() +\n  facet_wrap(~ Diabetes_binary) +\n  labs(title = \"Bar Plots of Difficulty Walking by Diabetes Diagnosis\",\n       x = \"Difficulty Walking\", fill = \"Diff Walking\")\n\n\n\n\n\n\n\n\nThere are more people who don’t have difficulty walking than those who do. When broken down by diabetes diagnosis both groups follow that same trend, however those with diabetes had proportionally more people who had difficulty walking than those without diabetes.\n\n\n\n\n# Get the counts of the Sex values\ndiabetes_data |&gt;\n  group_by(Sex) |&gt;\n  summarize(count = n())\n\n# A tibble: 2 × 2\n  Sex     count\n  &lt;fct&gt;   &lt;int&gt;\n1 Female 141974\n2 Male   111706\n\n\n\n# Bar plot of Sex\nggplot(diabetes_data, aes(Sex, fill = Sex)) +\n  geom_bar() +\n  labs(title = \"Bar Plot of Sex\")\n\n\n\n\n\n\n\n\n\n# Get two way contingency table of Sex and Diabetes_binary\ndiabetes_data |&gt;\n  group_by(Sex, Diabetes_binary) |&gt;\n  summarize(count = n(), .groups = \"drop\")\n\n# A tibble: 4 × 3\n  Sex    Diabetes_binary  count\n  &lt;fct&gt;  &lt;fct&gt;            &lt;int&gt;\n1 Female No              123563\n2 Female Yes              18411\n3 Male   No               94771\n4 Male   Yes              16935\n\n\n\n# Plot of Sex by Diabetes_binary\nggplot(diabetes_data, aes(Sex, fill = Sex)) +\n  geom_bar() +\n  facet_wrap(~ Diabetes_binary) + \n  labs(title = \"Bar Plots of Sex by Diabetes Diagnosis\")\n\n\n\n\n\n\n\n\nIn this dataset there are more females than males. When broken down by diabetes diagnosis that trend holds for both groups, however the diabetes group has a higher proportion of men than the group without diabetes.\n\n\n\n\n# Get the summary statistics of Age\nsummary(diabetes_data$Age)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   6.000   8.000   8.032  10.000  13.000 \n\n\n\n# Bar plot of Age\nggplot(diabetes_data, aes(Age, fill = factor(Age))) +\n  geom_bar() +\n  labs(title = \"Bar Plot of Age\", fill = \"Age\")\n\n\n\n\n\n\n\n\n\n# Get mean of Age for those with and without diabetes\ndiabetes_data |&gt;\n  group_by(Diabetes_binary) |&gt;\n  summarize(Mean = mean(Age), .groups = \"drop\")\n\n# A tibble: 2 × 2\n  Diabetes_binary  Mean\n  &lt;fct&gt;           &lt;dbl&gt;\n1 No               7.81\n2 Yes              9.38\n\n\n\n# Boxplots of Age by Diabetes_binary\nggplot(diabetes_data, aes(Diabetes_binary, Age, color = Diabetes_binary)) +\n  geom_boxplot() + \n  labs(title = \"Boxplots of Age by Diabetes Diagnosis\",\n       x = \"Diabetes Binary\", y = \"Age\", color = \"Diabetes Binary\")\n\n\n\n\n\n\n\n\n\n# Plot of Age by Diabetes_binary\nggplot(diabetes_data, aes(Age, fill = factor(Age))) +\n  geom_bar() +\n  facet_wrap(~ Diabetes_binary) +\n  labs(title = \"Bar Plots of Age by Diabetes Diagnosis\", fill = \"Age\")\n\n\n\n\n\n\n\n\nThe mean and median of the age groups are both roughly 8. Those who have diabetes are older on average than those without it. The distributions of the groups are roughly similar, though perhaps with proportionally fewer of the low age groups in the diabetes group.\n\n\n\n\n# Get the summary statistics of Income\nsummary(diabetes_data$Income)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   5.000   7.000   6.054   8.000   8.000 \n\n\n\n# Bar plot of Income\nggplot(diabetes_data, aes(Income, fill = factor(Income))) +\n  geom_bar() +\n  labs(title = \"Bar Plot of Income\", fill = \"Income\")\n\n\n\n\n\n\n\n\n\n# Get mean of Income for those with and without diabetes\ndiabetes_data |&gt;\n  group_by(Diabetes_binary) |&gt;\n  summarize(Mean = mean(Income), .groups = \"drop\")\n\n# A tibble: 2 × 2\n  Diabetes_binary  Mean\n  &lt;fct&gt;           &lt;dbl&gt;\n1 No               6.19\n2 Yes              5.21\n\n\n\n# Boxplots of Income by Diabetes_binary\nggplot(diabetes_data, aes(Diabetes_binary, Income, color = Diabetes_binary)) +\n  geom_boxplot() + \n  labs(title = \"Boxplots of Income by Diabetes Diagnosis\",\n       x = \"Diabetes Binary\", y = \"Income\", color = \"Diabetes Binary\")\n\n\n\n\n\n\n\n\n\n# Plot of Income by Diabetes_binary\nggplot(diabetes_data, aes(Income, fill = factor(Income))) +\n  geom_bar() +\n  facet_wrap(~ Diabetes_binary) +\n  labs(title = \"Bar Plots of Income by Diabetes Diagnosis\", fill = \"Income\")\n\n\n\n\n\n\n\n\nThe overall distribution of income is left skewed with a mean of roughly 6 and a median of 7. The average income of the people without diabetes is higher than those with it. As we can see in the distributions, the group with diabetes is a lot less steep of an increase, with the 8 income group being proportionally far less than in the group without diabetes.\nClick here for the Modeling Page"
  },
  {
    "objectID": "EDA.html#eda",
    "href": "EDA.html#eda",
    "title": "ST 558 Final Project EDA",
    "section": "",
    "text": "Click here for the Modeling Page"
  },
  {
    "objectID": "EDA.html#diabetes-health-indicators-dataset",
    "href": "EDA.html#diabetes-health-indicators-dataset",
    "title": "ST 558 Final Project EDA",
    "section": "",
    "text": "The Diabetes Health Indicators Dataset is from the BRFSS 2015 dataset on Kaggle. It is the Behavioral Risk Factor Surveillance System, which is a telephone survey collected annually by the CDC. Diabetes is a chronic disease that is prevalent in the United States, has a strongly negative impact on those with it, and has a major negative impact on the economy. Early diagnosis of the disease can allow individuals to make lifestyle changes and seek treatment, therefore it is important to build a model that can accurately predict it.\nThe focus of my project will be on how lifestyle and basic health variables impact whether or not someone has diabetes and if we are able to predict having diabetes based on these variables. The EDA will be used to explore these variables to look at their frequencies and distributions, as well as see how they relate to the response variable Diabetes_binary which expresses whether the person has diabetes or not. Modeling will be done to determine which model is best able to predict someone having diabetes based on which has the lowest log-loss.\n\n\nDiabetes_binary - binary variable where 0 is no diabetes and 1 is for prediabetes or diabetes. This is the response variable.\nHighBP - binary variable where 0 is not high and 1 is high blood pressure.\nSmoker - binary variable asking if the person has smoked at least 100 cigarettes (5 packs) in their entire life, where 0 is for no and 1 for yes.\nPhysActivity - binary variable for if they’ve done physical activity in the past 30 days, not including their job, where 0 is no and 1 is yes.\nFruits - binary variable for if they consume 1 or more fruit per day, where 0 is no and 1 is yes.\nVeggies - binary variable for if they consume 1 or more vegetable per day, where 0 is no and 1 is yes.\nHvyAlcoholConsump - binary variable for if men have &gt;= 14 drinks or women &gt;= 7 drinks per week, where 0 is no and 1 is yes.\nGenHlth - ordinal categorical variable describing their general health, where 1 is excellent, 2 is very good, 3 is good, 4 is fair, and 5 is poor.\nDiffWalk - binary variable for if they have serious difficulty walking or climbing stairs, where 0 is no and 1 is yes.\nSex - binary variable where 0 is female and 1 is male.\nAge - numerical variable from 1 to 13 representing different age categories where 1 is 18-24, 9 is 60-64, and 13 is 80 or older.\nIncome - numerical variable representing different income categories where 1 is less than $10,000, 5 is less than $35,000, and 8 is $75,000 or more.\n\n\n\n\n# Load in libraries\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.2.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(DescTools)\n\n\n# Load in Diabetes Health Indicators Dataset\ndiabetes_data &lt;- read_csv(\"API_Docker/diabetes_binary_health_indicators_BRFSS2015.csv\",\n                          show_col_types = FALSE)\n\n# Select only the variables used for my analysis and convert binary to factor\nanalysis_variables &lt;- c(\"Diabetes_binary\", \"HighBP\", \"Smoker\", \"PhysActivity\",\n                        \"Fruits\", \"Veggies\", \"HvyAlcoholConsump\", \"GenHlth\",\n                        \"DiffWalk\", \"Sex\", \"Age\", \"Income\")\n\n# When converting variables to factor, set levels and labels for those levels\ndiabetes_data &lt;- diabetes_data |&gt;\n  select(all_of(analysis_variables)) |&gt;\n  mutate(Diabetes_binary = factor(Diabetes_binary, \n                                  levels = c(\"0\", \"1\"), labels = c(\"No\", \"Yes\")),\n         HighBP = factor(HighBP,\n                         levels = c(\"0\", \"1\"), labels = c(\"Low\", \"High\")),\n         Smoker = factor(Smoker,\n                         levels = c(\"0\", \"1\"), labels = c(\"No\", \"Yes\")),\n         PhysActivity = factor(PhysActivity,\n                               levels = c(\"0\", \"1\"), labels = c(\"No\", \"Yes\")),\n         Fruits = factor(Fruits,\n                         levels = c(\"0\", \"1\"), labels = c(\"No\", \"Yes\")), \n         Veggies = factor(Veggies,\n                          levels = c(\"0\", \"1\"), labels = c(\"No\", \"Yes\")),\n         HvyAlcoholConsump = factor(HvyAlcoholConsump,\n                                    levels = c(\"0\", \"1\"), labels = c(\"No\", \"Yes\")),\n         DiffWalk = factor(DiffWalk,\n                           levels = c(\"0\", \"1\"), labels = c(\"No\", \"Yes\")),\n         Sex = factor(Sex,\n                      levels = c(\"0\", \"1\"), labels = c(\"Female\", \"Male\")))\n\ndiabetes_data\n\n# A tibble: 253,680 × 12\n   Diabetes_binary HighBP Smoker PhysActivity Fruits Veggies HvyAlcoholConsump\n   &lt;fct&gt;           &lt;fct&gt;  &lt;fct&gt;  &lt;fct&gt;        &lt;fct&gt;  &lt;fct&gt;   &lt;fct&gt;            \n 1 No              High   Yes    No           No     Yes     No               \n 2 No              Low    Yes    Yes          No     No      No               \n 3 No              High   No     No           Yes    No      No               \n 4 No              High   No     Yes          Yes    Yes     No               \n 5 No              High   No     Yes          Yes    Yes     No               \n 6 No              High   Yes    Yes          Yes    Yes     No               \n 7 No              High   Yes    No           No     No      No               \n 8 No              High   Yes    Yes          No     Yes     No               \n 9 Yes             High   Yes    No           Yes    Yes     No               \n10 No              Low    No     No           No     Yes     No               \n# ℹ 253,670 more rows\n# ℹ 5 more variables: GenHlth &lt;dbl&gt;, DiffWalk &lt;fct&gt;, Sex &lt;fct&gt;, Age &lt;dbl&gt;,\n#   Income &lt;dbl&gt;\n\n\n\n\n\n\nstr(diabetes_data)\n\ntibble [253,680 × 12] (S3: tbl_df/tbl/data.frame)\n $ Diabetes_binary  : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 2 1 ...\n $ HighBP           : Factor w/ 2 levels \"Low\",\"High\": 2 1 2 2 2 2 2 2 2 1 ...\n $ Smoker           : Factor w/ 2 levels \"No\",\"Yes\": 2 2 1 1 1 2 2 2 2 1 ...\n $ PhysActivity     : Factor w/ 2 levels \"No\",\"Yes\": 1 2 1 2 2 2 1 2 1 1 ...\n $ Fruits           : Factor w/ 2 levels \"No\",\"Yes\": 1 1 2 2 2 2 1 1 2 1 ...\n $ Veggies          : Factor w/ 2 levels \"No\",\"Yes\": 2 1 1 2 2 2 1 2 2 2 ...\n $ HvyAlcoholConsump: Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n $ GenHlth          : num [1:253680] 5 3 5 2 2 2 3 3 5 2 ...\n $ DiffWalk         : Factor w/ 2 levels \"No\",\"Yes\": 2 1 2 1 1 1 1 2 2 1 ...\n $ Sex              : Factor w/ 2 levels \"Female\",\"Male\": 1 1 1 1 1 2 1 1 1 2 ...\n $ Age              : num [1:253680] 9 7 9 11 11 10 9 11 9 8 ...\n $ Income           : num [1:253680] 3 1 8 6 4 8 7 4 1 3 ...\n\n\nThere are 253,680 observations with 1 response variable and 11 predictor variables.\n\n\n\n\n# Check for missing values\ncolSums(is.na(diabetes_data))\n\n  Diabetes_binary            HighBP            Smoker      PhysActivity \n                0                 0                 0                 0 \n           Fruits           Veggies HvyAlcoholConsump           GenHlth \n                0                 0                 0                 0 \n         DiffWalk               Sex               Age            Income \n                0                 0                 0                 0 \n\n\nThere are no missing values for these variables.\n\n\n\n\n# Get the counts of the Diabetes_binary values\ndiabetes_data |&gt;\n  group_by(Diabetes_binary) |&gt;\n  summarize(count = n())\n\n# A tibble: 2 × 2\n  Diabetes_binary  count\n  &lt;fct&gt;            &lt;int&gt;\n1 No              218334\n2 Yes              35346\n\n\n\n# Bar plot of Diabetes_binary\nggplot(diabetes_data, aes(Diabetes_binary, fill = Diabetes_binary)) +\n  geom_bar() + \n  labs(title = \"Bar Plot of Diabetes Binary\", \n       x = \"Diabetes Binary\", fill = \"Diabetes Binary\")\n\n\n\n\n\n\n\n\nAs you can see there are far more people who do not have diabetes than those who either have prediabetes or diabetes. Therefore when splitting the data into a training and test set, this response variable will be used as a strata to make sure that both sets have a roughly equal proportion of “No” and “Yes”.\n\n\n\n\n# Get the counts of the HighBP values\ndiabetes_data |&gt;\n  group_by(HighBP) |&gt;\n  summarize(count = n())\n\n# A tibble: 2 × 2\n  HighBP  count\n  &lt;fct&gt;   &lt;int&gt;\n1 Low    144851\n2 High   108829\n\n\n\n# Bar plot of HighBP\nggplot(diabetes_data, aes(HighBP, fill = HighBP)) +\n  geom_bar() +\n  labs(title = \"Bar Plot of High Blood Pressure\", \n       x = \"High Blood Pressure\", fill = \"High BP\")\n\n\n\n\n\n\n\n\n\n# Get two way contingency table of HighBP and Diabetes_binary\ndiabetes_data |&gt;\n  group_by(HighBP, Diabetes_binary) |&gt;\n  summarize(count = n(), .groups = \"drop\")\n\n# A tibble: 4 × 3\n  HighBP Diabetes_binary  count\n  &lt;fct&gt;  &lt;fct&gt;            &lt;int&gt;\n1 Low    No              136109\n2 Low    Yes               8742\n3 High   No               82225\n4 High   Yes              26604\n\n\n\n# Plot of HighBP by Diabetes_binary\nggplot(diabetes_data, aes(HighBP, fill = HighBP)) +\n  geom_bar() +\n  facet_wrap(~ Diabetes_binary) + \n  labs(title = \"Bar Plots of High Blood Pressure by Diabetes Diagnosis\",\n       x = \"High Blood Pressure\", fill = \"High BP\")\n\n\n\n\n\n\n\n\nOverall there are more people with low blood pressure, than with high blood pressure. However, when looking across the diabetes diagnosis, it appears that there are more people who have high blood pressure than low in the group that also has diabetes.\n\n\n\n\n# Get the counts of the Smoker values\ndiabetes_data |&gt;\n  group_by(Smoker) |&gt;\n  summarize(count = n())\n\n# A tibble: 2 × 2\n  Smoker  count\n  &lt;fct&gt;   &lt;int&gt;\n1 No     141257\n2 Yes    112423\n\n\n\n# Bar plot of Smoker\nggplot(diabetes_data, aes(Smoker, fill = Smoker)) +\n  geom_bar() + \n  labs(title = \"Bar Plot of Smoking Status\")\n\n\n\n\n\n\n\n\n\n# Get two way contingency table of Smoker and Diabetes_binary\ndiabetes_data |&gt;\n  group_by(Smoker, Diabetes_binary) |&gt;\n  summarize(count = n(), .groups = \"drop\")\n\n# A tibble: 4 × 3\n  Smoker Diabetes_binary  count\n  &lt;fct&gt;  &lt;fct&gt;            &lt;int&gt;\n1 No     No              124228\n2 No     Yes              17029\n3 Yes    No               94106\n4 Yes    Yes              18317\n\n\n\n# Plot of Smoker by Diabetes_binary\nggplot(diabetes_data, aes(Smoker, fill = Smoker)) +\n  geom_bar() +\n  facet_wrap(~ Diabetes_binary) + \n  labs(title = \"Bar Plots of Smoking Status by Diabetes Diagnosis\")\n\n\n\n\n\n\n\n\nOverall, there appears to be more people who are not smokers than those who are. However, when looking by diabetes diagnosis, the group that has diabetes have slightly more smokers than non-smokers.\n\n\n\n\n# Get the counts of the PhysActivity values\ndiabetes_data |&gt;\n  group_by(PhysActivity) |&gt;\n  summarize(count = n())\n\n# A tibble: 2 × 2\n  PhysActivity  count\n  &lt;fct&gt;         &lt;int&gt;\n1 No            61760\n2 Yes          191920\n\n\n\n# B plot of PhysActivity\nggplot(diabetes_data, aes(PhysActivity, fill = PhysActivity)) +\n  geom_bar() + \n  labs(title = \"Bar Plot of Physical Activity\", \n       x = \"Physical Activity\", fill = \"Physical Activity\")\n\n\n\n\n\n\n\n\n\n# Get two way contingency table of PhysActivity and Diabetes_binary\ndiabetes_data |&gt;\n  group_by(PhysActivity, Diabetes_binary) |&gt;\n  summarize(count = n(), .groups = \"drop\")\n\n# A tibble: 4 × 3\n  PhysActivity Diabetes_binary  count\n  &lt;fct&gt;        &lt;fct&gt;            &lt;int&gt;\n1 No           No               48701\n2 No           Yes              13059\n3 Yes          No              169633\n4 Yes          Yes              22287\n\n\n\n# Plot of PhysActivity by Diabetes_binary\nggplot(diabetes_data, aes(PhysActivity, fill = PhysActivity)) +\n  geom_bar() +\n  facet_wrap(~ Diabetes_binary) + \n  labs(title = \"Bar Plots of Physical Activity by Diabetes Diagnosis\",\n       x = \"Physical Activity\", fill = \"Physical Activity\")\n\n\n\n\n\n\n\n\nOverall there are far more people who get physical activity outside of work than those who don’t. Breaking it down by diabetes diagnosis, the same trend can be seen in both groups, however the proportion of those who exercise compared to those who don’t appears to be lower in the diabetes group.\n\n\n\n\n# Get the counts of the Fruits values\ndiabetes_data |&gt;\n  group_by(Fruits) |&gt;\n  summarize(count = n())\n\n# A tibble: 2 × 2\n  Fruits  count\n  &lt;fct&gt;   &lt;int&gt;\n1 No      92782\n2 Yes    160898\n\n\n\n# Bar plot of Fruits\nggplot(diabetes_data, aes(Fruits, fill = Fruits)) +\n  geom_bar() + \n  labs(title = \"Bar Plot of Fruit Consumption\")\n\n\n\n\n\n\n\n\n\n# Get two way contingency table of Fruits and Diabetes_binary\ndiabetes_data |&gt;\n  group_by(Fruits, Diabetes_binary) |&gt;\n  summarize(count = n(), .groups = \"drop\")\n\n# A tibble: 4 × 3\n  Fruits Diabetes_binary  count\n  &lt;fct&gt;  &lt;fct&gt;            &lt;int&gt;\n1 No     No               78129\n2 No     Yes              14653\n3 Yes    No              140205\n4 Yes    Yes              20693\n\n\n\n# Plot of Fruits by Diabetes_binary\nggplot(diabetes_data, aes(Fruits, fill = Fruits)) +\n  geom_bar() +\n  facet_wrap(~ Diabetes_binary) +\n  labs(title = \"Bar Plots of Fruit Consumption by Diabetes Diagnosis\")\n\n\n\n\n\n\n\n\nOverall there are more people who consume fruits each day than those that don’t. Breaking it down by diabetes diagnosis and the two groups look roughly similar. Fruit consumption likely doesn’t relate much to diabetes diagnosis.\n\n\n\n\n# Get the counts of the Veggies values\ndiabetes_data |&gt;\n  group_by(Veggies) |&gt;\n  summarize(count = n())\n\n# A tibble: 2 × 2\n  Veggies  count\n  &lt;fct&gt;    &lt;int&gt;\n1 No       47839\n2 Yes     205841\n\n\n\n# Bar plot of Veggies\nggplot(diabetes_data, aes(Veggies, fill = Veggies)) +\n  geom_bar() +\n  labs(title = \"Bar Plot of Vegetable Consumption\")\n\n\n\n\n\n\n\n\n\n# Get two way contingency table of Veggies and Diabetes_binary\ndiabetes_data |&gt;\n  group_by(Veggies, Diabetes_binary) |&gt;\n  summarize(count = n(), .groups = \"drop\")\n\n# A tibble: 4 × 3\n  Veggies Diabetes_binary  count\n  &lt;fct&gt;   &lt;fct&gt;            &lt;int&gt;\n1 No      No               39229\n2 No      Yes               8610\n3 Yes     No              179105\n4 Yes     Yes              26736\n\n\n\n# Plot of Veggies by Diabetes_binary\nggplot(diabetes_data, aes(Veggies, fill = Veggies)) +\n  geom_bar() +\n  facet_wrap(~ Diabetes_binary) + \n  labs(title = \"Bar Plots of Vegetable Consumption by Diabetes Diagnosis\")\n\n\n\n\n\n\n\n\nThere are more people who consume veggies each day than those that don’t. Breaking it down by diabetes diagnosis and the two groups look roughly similar. Veggie consumption likely doesn’t relate much to diabetes diagnosis.\n\n\n\n\n# Get the counts of the HvyAlcoholConsump values\ndiabetes_data |&gt;\n  group_by(HvyAlcoholConsump) |&gt;\n  summarize(count = n())\n\n# A tibble: 2 × 2\n  HvyAlcoholConsump  count\n  &lt;fct&gt;              &lt;int&gt;\n1 No                239424\n2 Yes                14256\n\n\n\n# Bar plot of HvyAlcoholConsump\nggplot(diabetes_data, aes(HvyAlcoholConsump, fill = HvyAlcoholConsump)) +\n  geom_bar() +\n  labs(title = \"Bar Plot of Heavy Alcohol Consumption\",\n       x = \"Heavy Alcohol Consumption\", fill = \"Hvy Alcohol Consump\")\n\n\n\n\n\n\n\n\n\n# Get two way contingency table of HvyAlcoholConsump and Diabetes_binary\ndiabetes_data |&gt;\n  group_by(HvyAlcoholConsump, Diabetes_binary) |&gt;\n  summarize(count = n(), .groups = \"drop\")\n\n# A tibble: 4 × 3\n  HvyAlcoholConsump Diabetes_binary  count\n  &lt;fct&gt;             &lt;fct&gt;            &lt;int&gt;\n1 No                No              204910\n2 No                Yes              34514\n3 Yes               No               13424\n4 Yes               Yes                832\n\n\n\n# Plot of HvyAlcoholConsump by Diabetes_binary\nggplot(diabetes_data, aes(HvyAlcoholConsump, fill = HvyAlcoholConsump)) +\n  geom_bar() +\n  facet_wrap(~ Diabetes_binary) + \n  labs(title = \"Bar Plots of Heavy Alcohol Consumption by Diabetes Diagnosis\",\n       x = \"Heavy Alcohol Consumption\", fill = \"Hvy Alcohol Consump\")\n\n\n\n\n\n\n\n\nThere are more people who don’t consume a heavy amount of alcohol than those who do. Breaking it down by diabetes diagnosis and the two groups look roughly similar. Heavy alcohol consumption likely doesn’t relate much to diabetes diagnosis.\n\n\n\n\n# Get the counts of the GenHlth values\ndiabetes_data |&gt;\n  group_by(GenHlth) |&gt;\n  summarize(count = n())\n\n# A tibble: 5 × 2\n  GenHlth count\n    &lt;dbl&gt; &lt;int&gt;\n1       1 45299\n2       2 89084\n3       3 75646\n4       4 31570\n5       5 12081\n\n\n\n# Get the summary statistics of GenHlth\nsummary(diabetes_data$GenHlth)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   2.000   2.000   2.511   3.000   5.000 \n\n\n\n# Bar plot of GenHlth\nggplot(diabetes_data, aes(GenHlth, fill = factor(GenHlth))) +\n  geom_bar() +\n  labs(title = \"Bar Plot of General Health\",\n       x = \"General Health (Excellent to Poor)\", fill = \"General Health\")\n\n\n\n\n\n\n\n\n\n# Get mean of GenHlth for those with and without diabetes\ndiabetes_data |&gt;\n  group_by(Diabetes_binary) |&gt;\n  summarize(Mean = mean(GenHlth), .groups = \"drop\")\n\n# A tibble: 2 × 2\n  Diabetes_binary  Mean\n  &lt;fct&gt;           &lt;dbl&gt;\n1 No               2.39\n2 Yes              3.29\n\n\n\n# Boxplots of GenHlth by Diabetes_binary\nggplot(diabetes_data, aes(Diabetes_binary, GenHlth, color = Diabetes_binary)) +\n  geom_boxplot() + \n  labs(title = \"Boxplots of General Health by Diabetes Diagnosis\",\n       x = \"Diabetes Binary\", y = \"General Health\", color = \"Diabetes Binary\")\n\n\n\n\n\n\n\n\n\n# Plot of GenHlth by Diabetes_binary\nggplot(diabetes_data, aes(GenHlth, fill = factor(GenHlth))) +\n  geom_bar() +\n  facet_wrap(~ Diabetes_binary) +\n  labs(title = \"Bar Plots of General Health by Diabetes Diagnosis\",\n       x = \"General Health\", fill = \"General Health\")\n\n\n\n\n\n\n\n\nOn average the general health of those with diabetes is worse than those without diabetes. As we can see in the distribution, there are proportionally far fewer people in excellent (1), very good (2), and good health (3), in the diabetes group.\n\n\n\n\n# Get the counts of the DiffWalk values\ndiabetes_data |&gt;\n  group_by(DiffWalk) |&gt;\n  summarize(count = n())\n\n# A tibble: 2 × 2\n  DiffWalk  count\n  &lt;fct&gt;     &lt;int&gt;\n1 No       211005\n2 Yes       42675\n\n\n\n# Plot of DiffWalk\nggplot(diabetes_data, aes(DiffWalk, fill = DiffWalk)) +\n  geom_bar() +\n  labs(title = \"Bar Plot of Difficulty Walking\", \n       x = \"Difficulty Walking\", fill = \"Diff Walking\")\n\n\n\n\n\n\n\n\n\n# Get two way contingency table of DiffWalk and Diabetes_binary\ndiabetes_data |&gt;\n  group_by(DiffWalk, Diabetes_binary) |&gt;\n  summarize(count = n(), .groups = \"drop\")\n\n# A tibble: 4 × 3\n  DiffWalk Diabetes_binary  count\n  &lt;fct&gt;    &lt;fct&gt;            &lt;int&gt;\n1 No       No              188780\n2 No       Yes              22225\n3 Yes      No               29554\n4 Yes      Yes              13121\n\n\n\n# Plot of DiffWalk by Diabetes_binary\nggplot(diabetes_data, aes(DiffWalk, fill = factor(DiffWalk))) +\n  geom_bar() +\n  facet_wrap(~ Diabetes_binary) +\n  labs(title = \"Bar Plots of Difficulty Walking by Diabetes Diagnosis\",\n       x = \"Difficulty Walking\", fill = \"Diff Walking\")\n\n\n\n\n\n\n\n\nThere are more people who don’t have difficulty walking than those who do. When broken down by diabetes diagnosis both groups follow that same trend, however those with diabetes had proportionally more people who had difficulty walking than those without diabetes.\n\n\n\n\n# Get the counts of the Sex values\ndiabetes_data |&gt;\n  group_by(Sex) |&gt;\n  summarize(count = n())\n\n# A tibble: 2 × 2\n  Sex     count\n  &lt;fct&gt;   &lt;int&gt;\n1 Female 141974\n2 Male   111706\n\n\n\n# Bar plot of Sex\nggplot(diabetes_data, aes(Sex, fill = Sex)) +\n  geom_bar() +\n  labs(title = \"Bar Plot of Sex\")\n\n\n\n\n\n\n\n\n\n# Get two way contingency table of Sex and Diabetes_binary\ndiabetes_data |&gt;\n  group_by(Sex, Diabetes_binary) |&gt;\n  summarize(count = n(), .groups = \"drop\")\n\n# A tibble: 4 × 3\n  Sex    Diabetes_binary  count\n  &lt;fct&gt;  &lt;fct&gt;            &lt;int&gt;\n1 Female No              123563\n2 Female Yes              18411\n3 Male   No               94771\n4 Male   Yes              16935\n\n\n\n# Plot of Sex by Diabetes_binary\nggplot(diabetes_data, aes(Sex, fill = Sex)) +\n  geom_bar() +\n  facet_wrap(~ Diabetes_binary) + \n  labs(title = \"Bar Plots of Sex by Diabetes Diagnosis\")\n\n\n\n\n\n\n\n\nIn this dataset there are more females than males. When broken down by diabetes diagnosis that trend holds for both groups, however the diabetes group has a higher proportion of men than the group without diabetes.\n\n\n\n\n# Get the summary statistics of Age\nsummary(diabetes_data$Age)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   6.000   8.000   8.032  10.000  13.000 \n\n\n\n# Bar plot of Age\nggplot(diabetes_data, aes(Age, fill = factor(Age))) +\n  geom_bar() +\n  labs(title = \"Bar Plot of Age\", fill = \"Age\")\n\n\n\n\n\n\n\n\n\n# Get mean of Age for those with and without diabetes\ndiabetes_data |&gt;\n  group_by(Diabetes_binary) |&gt;\n  summarize(Mean = mean(Age), .groups = \"drop\")\n\n# A tibble: 2 × 2\n  Diabetes_binary  Mean\n  &lt;fct&gt;           &lt;dbl&gt;\n1 No               7.81\n2 Yes              9.38\n\n\n\n# Boxplots of Age by Diabetes_binary\nggplot(diabetes_data, aes(Diabetes_binary, Age, color = Diabetes_binary)) +\n  geom_boxplot() + \n  labs(title = \"Boxplots of Age by Diabetes Diagnosis\",\n       x = \"Diabetes Binary\", y = \"Age\", color = \"Diabetes Binary\")\n\n\n\n\n\n\n\n\n\n# Plot of Age by Diabetes_binary\nggplot(diabetes_data, aes(Age, fill = factor(Age))) +\n  geom_bar() +\n  facet_wrap(~ Diabetes_binary) +\n  labs(title = \"Bar Plots of Age by Diabetes Diagnosis\", fill = \"Age\")\n\n\n\n\n\n\n\n\nThe mean and median of the age groups are both roughly 8. Those who have diabetes are older on average than those without it. The distributions of the groups are roughly similar, though perhaps with proportionally fewer of the low age groups in the diabetes group.\n\n\n\n\n# Get the summary statistics of Income\nsummary(diabetes_data$Income)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   5.000   7.000   6.054   8.000   8.000 \n\n\n\n# Bar plot of Income\nggplot(diabetes_data, aes(Income, fill = factor(Income))) +\n  geom_bar() +\n  labs(title = \"Bar Plot of Income\", fill = \"Income\")\n\n\n\n\n\n\n\n\n\n# Get mean of Income for those with and without diabetes\ndiabetes_data |&gt;\n  group_by(Diabetes_binary) |&gt;\n  summarize(Mean = mean(Income), .groups = \"drop\")\n\n# A tibble: 2 × 2\n  Diabetes_binary  Mean\n  &lt;fct&gt;           &lt;dbl&gt;\n1 No               6.19\n2 Yes              5.21\n\n\n\n# Boxplots of Income by Diabetes_binary\nggplot(diabetes_data, aes(Diabetes_binary, Income, color = Diabetes_binary)) +\n  geom_boxplot() + \n  labs(title = \"Boxplots of Income by Diabetes Diagnosis\",\n       x = \"Diabetes Binary\", y = \"Income\", color = \"Diabetes Binary\")\n\n\n\n\n\n\n\n\n\n# Plot of Income by Diabetes_binary\nggplot(diabetes_data, aes(Income, fill = factor(Income))) +\n  geom_bar() +\n  facet_wrap(~ Diabetes_binary) +\n  labs(title = \"Bar Plots of Income by Diabetes Diagnosis\", fill = \"Income\")\n\n\n\n\n\n\n\n\nThe overall distribution of income is left skewed with a mean of roughly 6 and a median of 7. The average income of the people without diabetes is higher than those with it. As we can see in the distributions, the group with diabetes is a lot less steep of an increase, with the 8 income group being proportionally far less than in the group without diabetes.\nClick here for the Modeling Page"
  },
  {
    "objectID": "Modeling.html",
    "href": "Modeling.html",
    "title": "ST 558 Final Project Modeling",
    "section": "",
    "text": "Now that we’ve explored the variables from the Diabetes Health Indicators Dataset, it’s time to create models with them. The goal is to be able to use lifestyle and basic health variables to predict whether or not someone has diabetes. Early diagnosis of diabetes can help individuals make lifestyle changes and seek treatment, therefore it is important to be able to accurately predict. For this analysis two types of models will be explored, a classification tree and a classification random forest. Both will use cross validation with tuning on a hyperparameter to select the best model from each, and then they will be compared on the test set to determine which is the overall best model.\n\n\n\n\n# Load in libraries\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.2.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.4.1 ──\n✔ broom        1.0.10     ✔ rsample      1.3.1 \n✔ dials        1.4.2      ✔ tailor       0.1.0 \n✔ infer        1.0.9      ✔ tune         2.0.1 \n✔ modeldata    1.5.1      ✔ workflows    1.3.0 \n✔ parsnip      1.3.3      ✔ workflowsets 1.1.1 \n✔ recipes      1.3.1      ✔ yardstick    1.3.2 \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n\n\n\n# Load in Diabetes Health Indicators Dataset\ndiabetes_data &lt;- read_csv(\"API_Docker/diabetes_binary_health_indicators_BRFSS2015.csv\",\n                          show_col_types = FALSE)\n\n# Select only the variables used for my analysis and convert binary to factor\nanalysis_variables &lt;- c(\"Diabetes_binary\", \"HighBP\", \"Smoker\", \"PhysActivity\",\n                        \"Fruits\", \"Veggies\", \"HvyAlcoholConsump\", \"GenHlth\",\n                        \"DiffWalk\", \"Sex\", \"Age\", \"Income\")\n\n# When converting variables to factor, set levels and labels for those levels\ndiabetes_data &lt;- diabetes_data |&gt;\n  select(all_of(analysis_variables)) |&gt;\n  mutate(Diabetes_binary = factor(Diabetes_binary, \n                                  levels = c(\"0\", \"1\"), labels = c(\"No\", \"Yes\")),\n         HighBP = factor(HighBP,\n                         levels = c(\"0\", \"1\"), labels = c(\"Low\", \"High\")),\n         Smoker = factor(Smoker,\n                         levels = c(\"0\", \"1\"), labels = c(\"No\", \"Yes\")),\n         PhysActivity = factor(PhysActivity,\n                               levels = c(\"0\", \"1\"), labels = c(\"No\", \"Yes\")),\n         Fruits = factor(Fruits,\n                         levels = c(\"0\", \"1\"), labels = c(\"No\", \"Yes\")), \n         Veggies = factor(Veggies,\n                          levels = c(\"0\", \"1\"), labels = c(\"No\", \"Yes\")),\n         HvyAlcoholConsump = factor(HvyAlcoholConsump,\n                                    levels = c(\"0\", \"1\"), labels = c(\"No\", \"Yes\")),\n         DiffWalk = factor(DiffWalk,\n                           levels = c(\"0\", \"1\"), labels = c(\"No\", \"Yes\")),\n         Sex = factor(Sex,\n                      levels = c(\"0\", \"1\"), labels = c(\"Female\", \"Male\")))\n\ndiabetes_data\n\n# A tibble: 253,680 × 12\n   Diabetes_binary HighBP Smoker PhysActivity Fruits Veggies HvyAlcoholConsump\n   &lt;fct&gt;           &lt;fct&gt;  &lt;fct&gt;  &lt;fct&gt;        &lt;fct&gt;  &lt;fct&gt;   &lt;fct&gt;            \n 1 No              High   Yes    No           No     Yes     No               \n 2 No              Low    Yes    Yes          No     No      No               \n 3 No              High   No     No           Yes    No      No               \n 4 No              High   No     Yes          Yes    Yes     No               \n 5 No              High   No     Yes          Yes    Yes     No               \n 6 No              High   Yes    Yes          Yes    Yes     No               \n 7 No              High   Yes    No           No     No      No               \n 8 No              High   Yes    Yes          No     Yes     No               \n 9 Yes             High   Yes    No           Yes    Yes     No               \n10 No              Low    No     No           No     Yes     No               \n# ℹ 253,670 more rows\n# ℹ 5 more variables: GenHlth &lt;dbl&gt;, DiffWalk &lt;fct&gt;, Sex &lt;fct&gt;, Age &lt;dbl&gt;,\n#   Income &lt;dbl&gt;\n\n\n\n\n\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Get training split with 70% of the data, and test split with 30%\n# Stratified on the response variable Diabetes_binary\ndiabetes_split &lt;- initial_split(diabetes_data, prop = 0.7, \n                                strata = Diabetes_binary)\ndiabetes_train &lt;- training(diabetes_split)\ndiabetes_test &lt;- testing(diabetes_split)\n\n# Create 5 fold Cross Validation splits\ndiabetes_folds &lt;- vfold_cv(diabetes_train, v = 5)\n\n\n\n\n\n\nA classification tree model splits the data into regions by dividing at certain points in the predictor variables. The tree then uses the most prevalent class in each region as its prediction. It decides which splits to make by using recursive binary splitting which is a greedy algorithm. It checks every possible value of each predictor to find the squared error loss based on splitting the data around that point. It chooses the split that minimizes this squared error loss the most. Each split then becomes its own path in the tree and more splits are done on each until reaching some stopping criterion - either tree depth (the max number of splits allowed) or min_n (the minimum number of observations left in each region).\n\n# Diabetes binary as the response and all other variables as predictors\n# normalize variables to be on same scale\ndiabetes_tree_rec &lt;- recipe(Diabetes_binary ~ ., data = diabetes_data) |&gt;\n  step_normalize(all_numeric())\n\n\n# classification decision tree model, cost complexity will be tuned\ndiabetes_tree_mod &lt;- decision_tree(tree_depth = 30,\n                                   min_n = 20,\n                                   cost_complexity = tune()) |&gt;\n  set_engine(\"rpart\") |&gt;\n  set_mode(\"classification\")\n\n\n# Create classification tree workflow\ndiabetes_tree_wkf &lt;- workflow() |&gt;\n  add_recipe(diabetes_tree_rec) |&gt;\n  add_model(diabetes_tree_mod)\n\n\n# Grid to tune cost compexity across 10 different values\ndiabetes_tree_grid &lt;- grid_regular(cost_complexity(),\n                                   levels = 10)\n\n# Use a tuning grid with the cross validation folds\n# using log loss as metric\ndiabetes_tree_fits &lt;- diabetes_tree_wkf |&gt;\n  tune_grid(resamples = diabetes_folds,\n            grid = diabetes_tree_grid,\n            metrics = metric_set(mn_log_loss))\n\n# Sort the fits by lowest to highest log loss\ndiabetes_tree_fits |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"mn_log_loss\") |&gt;\n  arrange(mean)\n\n# A tibble: 10 × 7\n   cost_complexity .metric     .estimator  mean     n std_err .config         \n             &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;           \n 1    0.0000000001 mn_log_loss binary     0.348     5 0.00301 pre0_mod01_post0\n 2    0.000000001  mn_log_loss binary     0.348     5 0.00301 pre0_mod02_post0\n 3    0.00000001   mn_log_loss binary     0.348     5 0.00301 pre0_mod03_post0\n 4    0.0000001    mn_log_loss binary     0.348     5 0.00301 pre0_mod04_post0\n 5    0.000001     mn_log_loss binary     0.348     5 0.00301 pre0_mod05_post0\n 6    0.00001      mn_log_loss binary     0.349     5 0.00321 pre0_mod06_post0\n 7    0.0001       mn_log_loss binary     0.359     5 0.00165 pre0_mod07_post0\n 8    0.001        mn_log_loss binary     0.404     5 0.00158 pre0_mod08_post0\n 9    0.01         mn_log_loss binary     0.404     5 0.00158 pre0_mod09_post0\n10    0.1          mn_log_loss binary     0.404     5 0.00158 pre0_mod10_post0\n\n\n\n# Select the fit with the lowest log loss\ndiabetes_tree_best_params &lt;- select_best(diabetes_tree_fits, \n                                         metric = \"mn_log_loss\")\n\ndiabetes_tree_best_params\n\n# A tibble: 1 × 2\n  cost_complexity .config         \n            &lt;dbl&gt; &lt;chr&gt;           \n1    0.0000000001 pre0_mod01_post0\n\n\n\n# Using the original workflow finalize the model with the best params\ndiabetes_tree_final_wkf &lt;- diabetes_tree_wkf |&gt;\n  finalize_workflow(diabetes_tree_best_params)\n\n# Fit the model on the entire training set and test on test set\ndiabetes_tree_final_fit &lt;- diabetes_tree_final_wkf |&gt;\n  last_fit(diabetes_split, metrics = metric_set(mn_log_loss))\n\n# Show how best classification tree performed on the test set\ndiabetes_tree_final_fit |&gt;\n  collect_metrics()\n\n# A tibble: 1 × 4\n  .metric     .estimator .estimate .config        \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;          \n1 mn_log_loss binary         0.347 pre0_mod0_post0\n\n\n\n\n\nA classification random forest is an ensemble model of many smaller classification decision trees that each work like I explained above. Bootstrapping is when we treat the sample like a population and resample from it with replacement. This in effect creates a sampling distribution. Each bootstrap sample is used to train a decision tree and then all of the tree predictions are aggregated to get a final prediction. So it’s better than a single tree because it helps account for the variability in any given sample to create a more robust model fit. In the random forest, the individual decision trees are also limited to randomly selecting a subset of the predictors (using the mtry parameter) and creating models based off of them. This helps prevent certain variables from taking over and there being multicollinearity between the trees. The models are forced to be more diverse which makes them more robust to new data.\n\n# Diabetes binary as the response and all other variables as predictors\n# normalize variables to be on same scale\ndiabetes_rf_rec &lt;- recipe(Diabetes_binary ~ HighBP + Smoker + PhysActivity + \n                            Fruits + Veggies + HvyAlcoholConsump + \n                            GenHlth + DiffWalk + Sex + Age + Income, \n                          data = diabetes_data) |&gt;\n  step_normalize(all_numeric())\n\n\n# classification random forest model, tuning number of params to select from for\n# each tree, also setting impurity to measure feature importance\ndiabetes_rf_mod &lt;- rand_forest(mtry = tune(),\n                               trees = 100) |&gt;\n  set_engine(\"ranger\", importance = \"impurity\") |&gt;\n  set_mode(\"classification\")\n\n\n# Create classification random forest workflow\ndiabetes_rf_wkf &lt;- workflow() |&gt;\n  add_recipe(diabetes_rf_rec) |&gt;\n  add_model(diabetes_rf_mod)\n\n\n# Grid to tune mtry (from 1 to 11 variables) across 5 different values\ndiabetes_rf_grid &lt;- grid_regular(mtry(range = c(1, 11)),\n                                 levels = 5)\n\n# Use a tuning grid with the cross validation folds\n# using log loss as metric\ndiabetes_rf_fit &lt;- diabetes_rf_wkf |&gt;\n  tune_grid(resamples = diabetes_folds,\n            grid = diabetes_rf_grid,\n            metrics = metric_set(mn_log_loss))\n\n# Sort the fits by lowest to highest log loss\ndiabetes_rf_fit |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"mn_log_loss\") |&gt;\n  arrange(mean)\n\n# A tibble: 5 × 7\n   mtry .metric     .estimator  mean     n std_err .config        \n  &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;          \n1     3 mn_log_loss binary     0.331     5 0.00117 pre0_mod2_post0\n2     6 mn_log_loss binary     0.349     5 0.00203 pre0_mod3_post0\n3     1 mn_log_loss binary     0.350     5 0.00118 pre0_mod1_post0\n4     8 mn_log_loss binary     0.373     5 0.00215 pre0_mod4_post0\n5    11 mn_log_loss binary     0.477     5 0.00371 pre0_mod5_post0\n\n\n\n# Select the fit with the lowest log loss\ndiabetes_rf_best_params &lt;- select_best(diabetes_rf_fit, metric = \"mn_log_loss\") \n\n# Using the original workflow finalize the model with the best params\ndiabetes_rf_final_wkf &lt;- diabetes_rf_wkf |&gt;\n  finalize_workflow(diabetes_rf_best_params) \n\n# Fit the model on the entire training set and test on test set\ndiabetes_rf_final_fit &lt;- diabetes_rf_final_wkf |&gt;\n  last_fit(diabetes_split, metrics = metric_set(mn_log_loss))\n\n# Show how best classification random forest performed on the test set\ndiabetes_rf_final_fit |&gt;\n  collect_metrics()\n\n# A tibble: 1 × 4\n  .metric     .estimator .estimate .config        \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;          \n1 mn_log_loss binary         0.329 pre0_mod0_post0\n\n\n\n\n\n\n\n# Compare the best classification decision tree and random forest\nrbind(\n  diabetes_tree_final_fit |&gt;\n    collect_metrics() |&gt;\n    mutate(Model = \"Tree\", .before = \".metric\"),\n  diabetes_rf_final_fit |&gt;\n    collect_metrics() |&gt;\n    mutate(Model = \"RF\", .before = \".metric\")\n)\n\n# A tibble: 2 × 5\n  Model .metric     .estimator .estimate .config        \n  &lt;chr&gt; &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;          \n1 Tree  mn_log_loss binary         0.347 pre0_mod0_post0\n2 RF    mn_log_loss binary         0.329 pre0_mod0_post0\n\n\nThe classification random forest performed best on the test set, therefore it is the overall best model.\n\n# Fit best model on full dataset\ndiabetes_best_model &lt;- diabetes_rf_final_wkf |&gt;\n  fit(diabetes_data)\n\ndiabetes_best_model\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n1 Recipe Step\n\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, mtry = min_cols(~3L,      x), num.trees = ~100, importance = ~\"impurity\", num.threads = 1,      verbose = FALSE, seed = sample.int(10^5, 1), probability = TRUE) \n\nType:                             Probability estimation \nNumber of trees:                  100 \nSample size:                      253680 \nNumber of independent variables:  11 \nMtry:                             3 \nTarget node size:                 10 \nVariable importance mode:         impurity \nSplitrule:                        gini \nOOB prediction error (Brier s.):  0.1021353 \n\n\n\n# get the best model fit\ndiabetes_rf_final_model &lt;- extract_fit_engine(diabetes_best_model)\n\n# Plot the variable importance from the best model\ndiabetes_rf_final_model$variable.importance |&gt;\n  enframe(name = \"Variable\", value = \"Importance\") |&gt;\n  arrange(Importance) |&gt;\n  mutate(Variable = factor(Variable, levels = Variable)) |&gt;\n  ggplot(aes(x = Importance, y = Variable)) +\n  geom_bar(stat = \"identity\")"
  },
  {
    "objectID": "Modeling.html#load-the-data",
    "href": "Modeling.html#load-the-data",
    "title": "ST 558 Final Project Modeling",
    "section": "",
    "text": "# Load in libraries\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.2.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.4.1 ──\n✔ broom        1.0.10     ✔ rsample      1.3.1 \n✔ dials        1.4.2      ✔ tailor       0.1.0 \n✔ infer        1.0.9      ✔ tune         2.0.1 \n✔ modeldata    1.5.1      ✔ workflows    1.3.0 \n✔ parsnip      1.3.3      ✔ workflowsets 1.1.1 \n✔ recipes      1.3.1      ✔ yardstick    1.3.2 \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n\n\n\n# Load in Diabetes Health Indicators Dataset\ndiabetes_data &lt;- read_csv(\"API_Docker/diabetes_binary_health_indicators_BRFSS2015.csv\",\n                          show_col_types = FALSE)\n\n# Select only the variables used for my analysis and convert binary to factor\nanalysis_variables &lt;- c(\"Diabetes_binary\", \"HighBP\", \"Smoker\", \"PhysActivity\",\n                        \"Fruits\", \"Veggies\", \"HvyAlcoholConsump\", \"GenHlth\",\n                        \"DiffWalk\", \"Sex\", \"Age\", \"Income\")\n\n# When converting variables to factor, set levels and labels for those levels\ndiabetes_data &lt;- diabetes_data |&gt;\n  select(all_of(analysis_variables)) |&gt;\n  mutate(Diabetes_binary = factor(Diabetes_binary, \n                                  levels = c(\"0\", \"1\"), labels = c(\"No\", \"Yes\")),\n         HighBP = factor(HighBP,\n                         levels = c(\"0\", \"1\"), labels = c(\"Low\", \"High\")),\n         Smoker = factor(Smoker,\n                         levels = c(\"0\", \"1\"), labels = c(\"No\", \"Yes\")),\n         PhysActivity = factor(PhysActivity,\n                               levels = c(\"0\", \"1\"), labels = c(\"No\", \"Yes\")),\n         Fruits = factor(Fruits,\n                         levels = c(\"0\", \"1\"), labels = c(\"No\", \"Yes\")), \n         Veggies = factor(Veggies,\n                          levels = c(\"0\", \"1\"), labels = c(\"No\", \"Yes\")),\n         HvyAlcoholConsump = factor(HvyAlcoholConsump,\n                                    levels = c(\"0\", \"1\"), labels = c(\"No\", \"Yes\")),\n         DiffWalk = factor(DiffWalk,\n                           levels = c(\"0\", \"1\"), labels = c(\"No\", \"Yes\")),\n         Sex = factor(Sex,\n                      levels = c(\"0\", \"1\"), labels = c(\"Female\", \"Male\")))\n\ndiabetes_data\n\n# A tibble: 253,680 × 12\n   Diabetes_binary HighBP Smoker PhysActivity Fruits Veggies HvyAlcoholConsump\n   &lt;fct&gt;           &lt;fct&gt;  &lt;fct&gt;  &lt;fct&gt;        &lt;fct&gt;  &lt;fct&gt;   &lt;fct&gt;            \n 1 No              High   Yes    No           No     Yes     No               \n 2 No              Low    Yes    Yes          No     No      No               \n 3 No              High   No     No           Yes    No      No               \n 4 No              High   No     Yes          Yes    Yes     No               \n 5 No              High   No     Yes          Yes    Yes     No               \n 6 No              High   Yes    Yes          Yes    Yes     No               \n 7 No              High   Yes    No           No     No      No               \n 8 No              High   Yes    Yes          No     Yes     No               \n 9 Yes             High   Yes    No           Yes    Yes     No               \n10 No              Low    No     No           No     Yes     No               \n# ℹ 253,670 more rows\n# ℹ 5 more variables: GenHlth &lt;dbl&gt;, DiffWalk &lt;fct&gt;, Sex &lt;fct&gt;, Age &lt;dbl&gt;,\n#   Income &lt;dbl&gt;"
  },
  {
    "objectID": "Modeling.html#split-the-data",
    "href": "Modeling.html#split-the-data",
    "title": "ST 558 Final Project Modeling",
    "section": "",
    "text": "# Set seed for reproducibility\nset.seed(123)\n\n# Get training split with 70% of the data, and test split with 30%\n# Stratified on the response variable Diabetes_binary\ndiabetes_split &lt;- initial_split(diabetes_data, prop = 0.7, \n                                strata = Diabetes_binary)\ndiabetes_train &lt;- training(diabetes_split)\ndiabetes_test &lt;- testing(diabetes_split)\n\n# Create 5 fold Cross Validation splits\ndiabetes_folds &lt;- vfold_cv(diabetes_train, v = 5)"
  },
  {
    "objectID": "Modeling.html#create-models",
    "href": "Modeling.html#create-models",
    "title": "ST 558 Final Project Modeling",
    "section": "",
    "text": "A classification tree model splits the data into regions by dividing at certain points in the predictor variables. The tree then uses the most prevalent class in each region as its prediction. It decides which splits to make by using recursive binary splitting which is a greedy algorithm. It checks every possible value of each predictor to find the squared error loss based on splitting the data around that point. It chooses the split that minimizes this squared error loss the most. Each split then becomes its own path in the tree and more splits are done on each until reaching some stopping criterion - either tree depth (the max number of splits allowed) or min_n (the minimum number of observations left in each region).\n\n# Diabetes binary as the response and all other variables as predictors\n# normalize variables to be on same scale\ndiabetes_tree_rec &lt;- recipe(Diabetes_binary ~ ., data = diabetes_data) |&gt;\n  step_normalize(all_numeric())\n\n\n# classification decision tree model, cost complexity will be tuned\ndiabetes_tree_mod &lt;- decision_tree(tree_depth = 30,\n                                   min_n = 20,\n                                   cost_complexity = tune()) |&gt;\n  set_engine(\"rpart\") |&gt;\n  set_mode(\"classification\")\n\n\n# Create classification tree workflow\ndiabetes_tree_wkf &lt;- workflow() |&gt;\n  add_recipe(diabetes_tree_rec) |&gt;\n  add_model(diabetes_tree_mod)\n\n\n# Grid to tune cost compexity across 10 different values\ndiabetes_tree_grid &lt;- grid_regular(cost_complexity(),\n                                   levels = 10)\n\n# Use a tuning grid with the cross validation folds\n# using log loss as metric\ndiabetes_tree_fits &lt;- diabetes_tree_wkf |&gt;\n  tune_grid(resamples = diabetes_folds,\n            grid = diabetes_tree_grid,\n            metrics = metric_set(mn_log_loss))\n\n# Sort the fits by lowest to highest log loss\ndiabetes_tree_fits |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"mn_log_loss\") |&gt;\n  arrange(mean)\n\n# A tibble: 10 × 7\n   cost_complexity .metric     .estimator  mean     n std_err .config         \n             &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;           \n 1    0.0000000001 mn_log_loss binary     0.348     5 0.00301 pre0_mod01_post0\n 2    0.000000001  mn_log_loss binary     0.348     5 0.00301 pre0_mod02_post0\n 3    0.00000001   mn_log_loss binary     0.348     5 0.00301 pre0_mod03_post0\n 4    0.0000001    mn_log_loss binary     0.348     5 0.00301 pre0_mod04_post0\n 5    0.000001     mn_log_loss binary     0.348     5 0.00301 pre0_mod05_post0\n 6    0.00001      mn_log_loss binary     0.349     5 0.00321 pre0_mod06_post0\n 7    0.0001       mn_log_loss binary     0.359     5 0.00165 pre0_mod07_post0\n 8    0.001        mn_log_loss binary     0.404     5 0.00158 pre0_mod08_post0\n 9    0.01         mn_log_loss binary     0.404     5 0.00158 pre0_mod09_post0\n10    0.1          mn_log_loss binary     0.404     5 0.00158 pre0_mod10_post0\n\n\n\n# Select the fit with the lowest log loss\ndiabetes_tree_best_params &lt;- select_best(diabetes_tree_fits, \n                                         metric = \"mn_log_loss\")\n\ndiabetes_tree_best_params\n\n# A tibble: 1 × 2\n  cost_complexity .config         \n            &lt;dbl&gt; &lt;chr&gt;           \n1    0.0000000001 pre0_mod01_post0\n\n\n\n# Using the original workflow finalize the model with the best params\ndiabetes_tree_final_wkf &lt;- diabetes_tree_wkf |&gt;\n  finalize_workflow(diabetes_tree_best_params)\n\n# Fit the model on the entire training set and test on test set\ndiabetes_tree_final_fit &lt;- diabetes_tree_final_wkf |&gt;\n  last_fit(diabetes_split, metrics = metric_set(mn_log_loss))\n\n# Show how best classification tree performed on the test set\ndiabetes_tree_final_fit |&gt;\n  collect_metrics()\n\n# A tibble: 1 × 4\n  .metric     .estimator .estimate .config        \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;          \n1 mn_log_loss binary         0.347 pre0_mod0_post0\n\n\n\n\n\nA classification random forest is an ensemble model of many smaller classification decision trees that each work like I explained above. Bootstrapping is when we treat the sample like a population and resample from it with replacement. This in effect creates a sampling distribution. Each bootstrap sample is used to train a decision tree and then all of the tree predictions are aggregated to get a final prediction. So it’s better than a single tree because it helps account for the variability in any given sample to create a more robust model fit. In the random forest, the individual decision trees are also limited to randomly selecting a subset of the predictors (using the mtry parameter) and creating models based off of them. This helps prevent certain variables from taking over and there being multicollinearity between the trees. The models are forced to be more diverse which makes them more robust to new data.\n\n# Diabetes binary as the response and all other variables as predictors\n# normalize variables to be on same scale\ndiabetes_rf_rec &lt;- recipe(Diabetes_binary ~ HighBP + Smoker + PhysActivity + \n                            Fruits + Veggies + HvyAlcoholConsump + \n                            GenHlth + DiffWalk + Sex + Age + Income, \n                          data = diabetes_data) |&gt;\n  step_normalize(all_numeric())\n\n\n# classification random forest model, tuning number of params to select from for\n# each tree, also setting impurity to measure feature importance\ndiabetes_rf_mod &lt;- rand_forest(mtry = tune(),\n                               trees = 100) |&gt;\n  set_engine(\"ranger\", importance = \"impurity\") |&gt;\n  set_mode(\"classification\")\n\n\n# Create classification random forest workflow\ndiabetes_rf_wkf &lt;- workflow() |&gt;\n  add_recipe(diabetes_rf_rec) |&gt;\n  add_model(diabetes_rf_mod)\n\n\n# Grid to tune mtry (from 1 to 11 variables) across 5 different values\ndiabetes_rf_grid &lt;- grid_regular(mtry(range = c(1, 11)),\n                                 levels = 5)\n\n# Use a tuning grid with the cross validation folds\n# using log loss as metric\ndiabetes_rf_fit &lt;- diabetes_rf_wkf |&gt;\n  tune_grid(resamples = diabetes_folds,\n            grid = diabetes_rf_grid,\n            metrics = metric_set(mn_log_loss))\n\n# Sort the fits by lowest to highest log loss\ndiabetes_rf_fit |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"mn_log_loss\") |&gt;\n  arrange(mean)\n\n# A tibble: 5 × 7\n   mtry .metric     .estimator  mean     n std_err .config        \n  &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;          \n1     3 mn_log_loss binary     0.331     5 0.00117 pre0_mod2_post0\n2     6 mn_log_loss binary     0.349     5 0.00203 pre0_mod3_post0\n3     1 mn_log_loss binary     0.350     5 0.00118 pre0_mod1_post0\n4     8 mn_log_loss binary     0.373     5 0.00215 pre0_mod4_post0\n5    11 mn_log_loss binary     0.477     5 0.00371 pre0_mod5_post0\n\n\n\n# Select the fit with the lowest log loss\ndiabetes_rf_best_params &lt;- select_best(diabetes_rf_fit, metric = \"mn_log_loss\") \n\n# Using the original workflow finalize the model with the best params\ndiabetes_rf_final_wkf &lt;- diabetes_rf_wkf |&gt;\n  finalize_workflow(diabetes_rf_best_params) \n\n# Fit the model on the entire training set and test on test set\ndiabetes_rf_final_fit &lt;- diabetes_rf_final_wkf |&gt;\n  last_fit(diabetes_split, metrics = metric_set(mn_log_loss))\n\n# Show how best classification random forest performed on the test set\ndiabetes_rf_final_fit |&gt;\n  collect_metrics()\n\n# A tibble: 1 × 4\n  .metric     .estimator .estimate .config        \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;          \n1 mn_log_loss binary         0.329 pre0_mod0_post0"
  },
  {
    "objectID": "Modeling.html#introduction",
    "href": "Modeling.html#introduction",
    "title": "ST 558 Final Project Modeling",
    "section": "",
    "text": "Now that we’ve explored the variables from the Diabetes Health Indicators Dataset, it’s time to create models with them. The goal is to be able to use lifestyle and basic health variables to predict whether or not someone has diabetes. Early diagnosis of diabetes can help individuals make lifestyle changes and seek treatment, therefore it is important to be able to accurately predict. For this analysis two types of models will be explored, a classification tree and a classification random forest. Both will use cross validation with tuning on a hyperparameter to select the best model from each, and then they will be compared on the test set to determine which is the overall best model."
  },
  {
    "objectID": "Modeling.html#final-model-selection",
    "href": "Modeling.html#final-model-selection",
    "title": "ST 558 Final Project Modeling",
    "section": "",
    "text": "# Compare the best classification decision tree and random forest\nrbind(\n  diabetes_tree_final_fit |&gt;\n    collect_metrics() |&gt;\n    mutate(Model = \"Tree\", .before = \".metric\"),\n  diabetes_rf_final_fit |&gt;\n    collect_metrics() |&gt;\n    mutate(Model = \"RF\", .before = \".metric\")\n)\n\n# A tibble: 2 × 5\n  Model .metric     .estimator .estimate .config        \n  &lt;chr&gt; &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;          \n1 Tree  mn_log_loss binary         0.347 pre0_mod0_post0\n2 RF    mn_log_loss binary         0.329 pre0_mod0_post0\n\n\nThe classification random forest performed best on the test set, therefore it is the overall best model.\n\n# Fit best model on full dataset\ndiabetes_best_model &lt;- diabetes_rf_final_wkf |&gt;\n  fit(diabetes_data)\n\ndiabetes_best_model\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n1 Recipe Step\n\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, mtry = min_cols(~3L,      x), num.trees = ~100, importance = ~\"impurity\", num.threads = 1,      verbose = FALSE, seed = sample.int(10^5, 1), probability = TRUE) \n\nType:                             Probability estimation \nNumber of trees:                  100 \nSample size:                      253680 \nNumber of independent variables:  11 \nMtry:                             3 \nTarget node size:                 10 \nVariable importance mode:         impurity \nSplitrule:                        gini \nOOB prediction error (Brier s.):  0.1021353 \n\n\n\n# get the best model fit\ndiabetes_rf_final_model &lt;- extract_fit_engine(diabetes_best_model)\n\n# Plot the variable importance from the best model\ndiabetes_rf_final_model$variable.importance |&gt;\n  enframe(name = \"Variable\", value = \"Importance\") |&gt;\n  arrange(Importance) |&gt;\n  mutate(Variable = factor(Variable, levels = Variable)) |&gt;\n  ggplot(aes(x = Importance, y = Variable)) +\n  geom_bar(stat = \"identity\")"
  }
]